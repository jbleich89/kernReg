% Generated by roxygen2 (4.0.1): do not edit by hand
\name{eval_winning_lr_model_on_test_data}
\alias{eval_winning_lr_model_on_test_data}
\title{Evaluate Test Data}
\usage{
eval_winning_lr_model_on_test_data(explore_kpclr_obj,
  use_validation_data = TRUE)
}
\arguments{
\item{explore_kpclr_obj}{This object is built from \code{explore_kplcr_models}. We assume the user has updated
this object with a satisfactory model by settings \code{winning_kernel_num} to denote
which kernel is selected for the final model and setting \code{winning_rho_num} to
denote which proportion of the variance of the kernel matrix is selected for the
final model.}

\item{use_validation_data}{Should we use the validation data along with the training data. Default is \code{TRUE}.
                                    From our experience, leaving this \code{FALSE} allows models with better out-of-sample
                                    error ratios (number of false negatives to false positives or vice versa). The tradeoff
                                    is a larger overall misclassification error because the model is build with the sample size
                                    of the training data, not the training plus the validation data.}
}
\value{
An expanded \code{explore_kpclr} list object with new entries
									that contain information about the performance of the final model on the
									test data: \code{test_confusion}, the confusion matrix of the test data;
									\code{test_confusion_proportions}, the confusion matrix of the test data
									as proportions of the number of test observations; \code{test_misclassification_error},
									the total misclassification error of the test data and \code{test_weighted_cost},
									the cost of the errors made as defined by the \code{fn_cost} and \code{fp_cost}
									specified by the user when constructing the model via \code{explore_kplcr_models}.
}
\description{
After a "satisfactory" model is selected by the user using the \code{explore_kpcr_models} function,
we now predict on the test data to get a glimpse into this model's future out-of-sample performance.
Warning: once this is done, you cannot "go back" and "try" to assess performance on new kernels as this
would then be snooping. Run this function when you are ready to close the books on this data set and never
look back.
}
\examples{
\dontrun{
#pull the predictor matrix and dummify the response from the Boston Housing Data
data(Boston)
y = ifelse(Boston$medv > median(Boston$medv), 1, 0)
Boston$medv = NULL
X = as.matrix(Boston)
#now explore kernel models using the default kernel list and misclassification costs.
#Use parallelization for speed.
explore_kpclr_obj = explore_kpclr_models(X, y, num_cores = 4)
#now we plot to see how the models built on the training data performed on the validation data
plot(explore_kpclr_obj)
#suppose we choose the 2nd kernel and the 10th rho
explore_kpclr_obj = set_desired_model(explore_kpclr_obj, 2, 10)
#we can re-plot to ensure the chosen model is properly marked with a vertical line
plot(explore_kpclr_obj)
#now we build this model using the training and validation data and assess
#out-of-sample performance by predicting on the test data
explore_kpclr_obj = eval_winning_lr_model_on_test_data(explore_kpclr_obj)
#show results to console
explore_kpclr_obj
}
}
\author{
Adam Kapelner and Justin Bleich
}
\seealso{
\code{\link{explore_kpclr_models}}
}

