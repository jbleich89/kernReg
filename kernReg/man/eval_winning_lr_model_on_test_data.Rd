% Generated by roxygen2 (4.0.1): do not edit by hand
\name{eval_winning_lr_model_on_test_data}
\alias{eval_winning_lr_model_on_test_data}
\title{Evaluate Test Data}
\usage{
eval_winning_lr_model_on_test_data(explore_kpclr_obj)
}
\arguments{
\item{explore_kpclr_obj}{This object is built from \code{explore_kplcr_models}. We assume the user has updated
                                    this object with a satisfactory model by settings \code{winning_kernel_num} to denote
                                    which kernel is selected for the final model and setting \code{winning_rho_num} to
                                    denote which proportion of the variance of the kernel matrix is selected for the
                                    final model.}
}
\value{
An expanded \code{explore_kpclr} list object with new entries
									that contain information about the performance of the final model on the
									test data: \code{test_confusion}, the confusion matrix of the test data;
									\code{test_confusion_proportions}, the confusion matrix of the test data
									as proportions of the number of test observations; \code{test_misclassification_error},
									the total misclassification error of the test data and \code{test_weighted_cost},
									the cost of the errors made as defined by the \code{fn_cost} and \code{fp_cost}
									specified by the user when constructing the model via \code{explore_kplcr_models}.
}
\description{
After a "satisfactory" model is selected by the user using the \code{explore_kpcr_models} function,
we now predict on the test data to get a glimpse into this model's future out-of-sample performance.
Warning: once this is done, you cannot "go back" and "try" to assess performance on new kernels as this
would then be snooping. Run this function when you are ready to close the books on this data set and never
look back.
}
\author{
Adam Kapelner and Justin Bleich
}
\seealso{
code{explore_kplcr_models}
}

